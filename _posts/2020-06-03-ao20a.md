---
title: An approximate KLD based experimental design for models with intractable likelihoods
abstract: 'Data collection is a critical step in statistical inference and data science,and
  the goal of statistical experimental design (ED) is to find the data collection
  setupthat can provide most information for the inference. In this work we consider
  a special type of ED problems where the likelihoods are not available in a closed
  form. In this case, the popular information-theoretic Kullback-Leibler divergence
  (KLD) based design criterioncan not be used directly, as it requires to evaluate
  the likelihood function. To address the issue, we derive a new utility function,which
  is a lower bound of the original KLD utility. This lower bound is expressed in terms
  of the summation of two or more entropies in the data space, and thus can be evaluated
  efficiently via entropy estimation methods.We provide several numerical examples
  to demonstrate the performance of the proposed method. '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: ao20a
month: 0
tex_title: An approximate KLD based experimental design for models with intractable
  likelihoods
firstpage: 3241
lastpage: 3251
page: 3241-3251
order: 3241
cycles: false
bibtex_author: Ao, Ziqiao and Li, Jinglai
author:
- given: Ziqiao
  family: Ao
- given: Jinglai
  family: Li
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/ao20a/ao20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/ao20a/ao20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
