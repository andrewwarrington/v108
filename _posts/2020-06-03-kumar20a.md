---
title: Regularized Autoencoders via Relaxed Injective Probability Flow
abstract: Invertible flow-based generative models are an effective method for learning
  to generate samples, while allowing for tractable likelihood computation and inference.
  However, the invertibility requirement restricts models to have the same latent
  dimensionality as the inputs. This imposes significant architectural, memory, and
  computational costs,  making them more challenging to scale than other classes of
  generative models such as Variational Autoencoders (VAEs). We propose a generative
  model based on probability flows that does away with the bijectivity requirement
  on the model and only assumes injectivity. This also provides another perspective
  on regularized autoencoders (RAEs), with our final objectives resembling RAEs with
  specific regularizers that are derived by lower bounding the probability flow objective.
  We empirically demonstrate the promise of the proposed model, improving over VAEs
  and AEs in terms of sample quality.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: kumar20a
month: 0
tex_title: Regularized Autoencoders via Relaxed Injective Probability Flow
firstpage: 4292
lastpage: 4301
page: 4292-4301
order: 4292
cycles: false
bibtex_author: Kumar, Abhishek and Poole, Ben and Murphy, Kevin
author:
- given: Abhishek
  family: Kumar
- given: Ben
  family: Poole
- given: Kevin
  family: Murphy
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/kumar20a/kumar20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/kumar20a/kumar20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
