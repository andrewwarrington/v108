---
title: Conservative Exploration in Reinforcement Learning
abstract: While learning in an unknown Markov Decision Process (MDP), an agent should
  trade off exploration to discover new information about the MDP, and exploitation
  of the current knowledge to maximize the reward. Although the agent will eventually
  learn a good or optimal policy, there is no guarantee on the quality of the intermediate
  policies. This lack of control is undesired in real-world applications where a minimum
  requirement is that the executed policies are guaranteed to perform at least as
  well as an existing baseline. In this paper, we introduce the notion of conservative
  exploration for average reward and finite horizon problems. We present two optimistic
  algorithms that guarantee (w.h.p.) that the conservative constraint is never violated
  during learning. We derive regret bounds showing that being conservative does not
  hinder the learning ability of these algorithms.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: garcelon20a
month: 0
tex_title: Conservative Exploration in Reinforcement Learning
firstpage: 1431
lastpage: 1441
page: 1431-1441
order: 1431
cycles: false
bibtex_author: Garcelon, Evrard and Ghavamzadeh, Mohammad and Lazaric, Alessandro
  and Pirotta, Matteo
author:
- given: Evrard
  family: Garcelon
- given: Mohammad
  family: Ghavamzadeh
- given: Alessandro
  family: Lazaric
- given: Matteo
  family: Pirotta
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/garcelon20a/garcelon20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/garcelon20a/garcelon20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
