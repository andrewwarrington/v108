---
title: Sketching Transformed Matrices with Applications to Natural Language Processing
abstract: 'Suppose we are given a large matrix $A=(a_{i,j})$ that cannot be stored
  in memory but is in a disk or is presented in a data stream. However, we need to
  compute a matrix decomposition of the entry-wisely transformed matrix, $f(A):=(f(a_{i,j}))$
  for some function $f$. Is it possible to do it in a space efficient way? Many machine
  learning applications indeed need to deal with such large transformed matrices,
  for example word embedding method in NLP needs to work with the pointwise mutual
  information (PMI) matrix, while the entrywise transformation makes it difficult
  to apply known linear algebraic tools. Existing approaches for this problem either
  need to store the whole matrix and perform the entry-wise transformation afterwards,
  which is space consuming or infeasible, or need to redesign the learning method,
  which is application specific and requires substantial remodeling.In this paper,
  we first propose a space-efficient sketching algorithm for computing the product
  of a given small matrix with the transformed matrix. It works for a general family
  of transformations with provable small error bounds and thus can be used as a primitive
  in downstream learning tasks. We then apply this primitive to two concrete applications:
  low-rank approximation and linear regressions. We show that our approach obtains
  small error and is efficient in both space and time. For instance, for a large $n\times
  n$ matrix $A$, we show that only $\tilde{O}(nk^3)$ space and a few scans over the
  matrix $A$ are needed to compute a rank-$k$ approximation of $\log(|A|+1)$ to a
  fixed accuracy. This is a nearly quadratic space improvement for small $k$. We complement
  our theoretical results with experiments of low-rank approximation on synthetic
  and real data. '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: liang20a
month: 0
tex_title: Sketching Transformed Matrices with Applications to Natural Language Processing
firstpage: 467
lastpage: 481
page: 467-481
order: 467
cycles: false
bibtex_author: Liang, Yingyu and Song, Zhao and Wang, Mengdi and Yang, Lin and Yang,
  Xin
author:
- given: Yingyu
  family: Liang
- given: Zhao
  family: Song
- given: Mengdi
  family: Wang
- given: Lin
  family: Yang
- given: Xin
  family: Yang
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/liang20a/liang20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/liang20a/liang20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
