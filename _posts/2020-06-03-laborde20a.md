---
title: 'A Lyapunov analysis for accelerated gradient methods: from deterministic to
  stochastic case'
abstract: Recent work by Su, Boyd and Candes made a connection between Nesterov’s
  accelerated gradient descent method and an ordinary differential equation (ODE).
  We show that this connection can be extended to the case of stochastic gradients,
  and develop Lyapunov function based convergence rates proof for Nesterov’s accelerated
  stochastic gradient descent. In the gradient case, we show Nesterov’s method arises
  as a straightforward discretization of a modified ODE. Established Lyapunov analysis
  is used to recover the accelerated rates of convergence in both continuous and discrete
  time. Moreover, the Lyapunov analysis can be extended to the case of stochastic
  gradients. The result is a unified approach to accelerationin both continuous and
  discrete time, and in for both stochastic and full gradients.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: laborde20a
month: 0
tex_title: 'A Lyapunov analysis for accelerated gradient methods: from deterministic
  to stochastic case'
firstpage: 602
lastpage: 612
page: 602-612
order: 602
cycles: false
bibtex_author: Laborde, Maxime and Oberman, Adam
author:
- given: Maxime
  family: Laborde
- given: Adam
  family: Oberman
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/laborde20a/laborde20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/laborde20a/laborde20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
