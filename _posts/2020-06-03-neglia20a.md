---
title: 'Decentralized gradient methods: does topology matter?'
abstract: 'Consensus-based distributed optimization methods have recently been advocated
  as alternatives to parameter server and ring all-reduce paradigms for large scale
  training of machine learning models. In this case, each worker maintains a local
  estimate of the optimal parameter vector and iteratively updates it by averaging
  the estimates obtained from its neighbors, and applying a correction on the basis
  of its local dataset. While theoretical results suggest that worker communication
  topology should have strong impact on the number of epochs needed to converge, previous
  experiments have shown the opposite conclusion. This paper sheds lights on this
  apparent contradiction and show how sparse topologies can lead to faster convergence
  even in the absence of communication delays. '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: neglia20a
month: 0
tex_title: 'Decentralized gradient methods: does topology matter?'
firstpage: 2348
lastpage: 2358
page: 2348-2358
order: 2348
cycles: false
bibtex_author: Neglia, Giovanni and Xu, Chuan and Towsley, Don and Calbi, Gianmarco
author:
- given: Giovanni
  family: Neglia
- given: Chuan
  family: Xu
- given: Don
  family: Towsley
- given: Gianmarco
  family: Calbi
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/neglia20a/neglia20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/neglia20a/neglia20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
