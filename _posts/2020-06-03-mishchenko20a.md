---
title: Revisiting Stochastic Extragradient
abstract: We fix a fundamental issue in the stochastic extragradient method by providing
  a new sampling strategy that is motivated by approximating implicit updates. Since
  the existing stochastic extragradient algorithm, called Mirror-Prox, of (Juditsky,
  2011) diverges on a simple bilinear problem when the domain is not bounded, we prove
  guarantees for solving variational inequality that go beyond existing settings.
  Furthermore, we illustrate numerically that the proposed variant converges faster
  than many other methods on several convex-concave saddle-point problems. We also
  discuss how extragradient can be applied to training Generative Adversarial Networks
  (GANs) and how it compares to other methods. Our experiments on GANs demonstrate
  that the introduced approach may make the training faster in terms of data passes,
  while its higher iteration complexity makes the advantage smaller.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: mishchenko20a
month: 0
tex_title: Revisiting Stochastic Extragradient
firstpage: 4573
lastpage: 4582
page: 4573-4582
order: 4573
cycles: false
bibtex_author: Mishchenko, Konstantin and Kovalev, Dmitry and Shulgin, Egor and Richtarik,
  Peter and Malitsky, Yura
author:
- given: Konstantin
  family: Mishchenko
- given: Dmitry
  family: Kovalev
- given: Egor
  family: Shulgin
- given: Peter
  family: Richtarik
- given: Yura
  family: Malitsky
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/mishchenko20a/mishchenko20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/mishchenko20a/mishchenko20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
