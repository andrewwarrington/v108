---
title: Value Preserving State-Action Abstractions
abstract: Abstraction can improve the sample efficiency of reinforcement learning.
  However, the process of abstraction inherently discards information, potentially
  compromising an agentâ€™s ability to represent high-value policies. To mitigate this,
  we here introduce combinations of state abstractions and options that are guaranteed
  to preserve representation of near-optimal policies. We first define $\phi$-relative
  options, a general formalism for analyzing the value loss of options paired with
  a state abstraction, and present necessary and sufficient conditions for $\phi$-relative
  options to preserve near-optimal behavior in any finite Markov Decision Process.
  We further show that, under appropriate assumptions, $\phi$-relative options can
  be composed to induce hierarchical abstractions that are also guaranteed to represent
  high-value policies.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: abel20a
month: 0
tex_title: Value Preserving State-Action Abstractions
firstpage: 1639
lastpage: 1650
page: 1639-1650
order: 1639
cycles: false
bibtex_author: Abel, David and Umbanhowar, Nate and Khetarpal, Khimya and Arumugam,
  Dilip and Precup, Doina and Littman, Michael
author:
- given: David
  family: Abel
- given: Nate
  family: Umbanhowar
- given: Khimya
  family: Khetarpal
- given: Dilip
  family: Arumugam
- given: Doina
  family: Precup
- given: Michael
  family: Littman
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/abel20a/abel20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/abel20a/abel20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
