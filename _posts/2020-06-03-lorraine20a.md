---
title: Optimizing Millions of Hyperparameters by Implicit Differentiation
abstract: We propose an algorithm for inexpensive gradient-based hyperparameter optimization
  that combines the implicit function theorem (IFT) with efficient inverse Hessian
  approximations. We present results about the relationship between the IFT and differentiating
  through optimization, motivating our algorithm.  We use the proposed approach to
  train modern network architectures with millions of weights and millions of hyper-parameters.
  For example, we learn a data-augmentation network—where every weight is a hyperparameter
  tuned for validation performance—outputting augmented training examples. Jointly
  tuning weights and hyper-parameters is only a few times more costly in memory and
  compute than standard training.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: lorraine20a
month: 0
tex_title: Optimizing Millions of Hyperparameters by Implicit Differentiation
firstpage: 1540
lastpage: 1552
page: 1540-1552
order: 1540
cycles: false
bibtex_author: Lorraine, Jonathan and Vicol, Paul and Duvenaud, David
author:
- given: Jonathan
  family: Lorraine
- given: Paul
  family: Vicol
- given: David
  family: Duvenaud
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/lorraine20a/lorraine20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/lorraine20a/lorraine20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
