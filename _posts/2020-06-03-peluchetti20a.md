---
title: Infinitely deep neural networks as diffusion processes
abstract: When the parameters are independently and identically distributed (initialized)
  neural networks exhibit undesirable properties that emerge as the number of layers
  increases, e.g. a vanishing dependency on the input and a concentration on restrictive
  families of functions including constant functions. We consider parameter distributions
  that shrink as the number of layers increases in order to recover well-behaved stochastic
  processes in the limit of infinite depth. This leads to set forth a link between
  infinitely deep residual networks and solutions to stochastic differential equations,
  i.e. diffusion processes. We show that these limiting processes do not suffer from
  the aforementioned issues and investigate their properties.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: peluchetti20a
month: 0
tex_title: Infinitely deep neural networks as diffusion processes
firstpage: 1126
lastpage: 1136
page: 1126-1136
order: 1126
cycles: false
bibtex_author: Peluchetti, Stefano and Favaro, Stefano
author:
- given: Stefano
  family: Peluchetti
- given: Stefano
  family: Favaro
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/peluchetti20a/peluchetti20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/peluchetti20a/peluchetti20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
