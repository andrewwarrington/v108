---
title: Neural Topic Model with Attention for Supervised Learning
abstract: Topic modeling utilizing neural variational inference has shown promising
  results recently. Unlike traditional Bayesian topic models, neural topic models
  use deep neural network to approximate the intractable marginal distribution and
  thus gain strong generalisation ability. However, neural topic models are unsupervised
  model. Directly using the document-specific topic proportions in downstream prediction
  tasks could lead to sub-optimal performance. This paper presents Topic Attention
  Model (TAM), a supervised neural topic model that integrates an attention recurrent
  neural network (RNN) model. We design a novel way to utilize document-specific topic
  proportions and global topic vectors learned from neural topic model in the attention
  mechanism. We also develop backpropagation inference method that allows for joint
  model optimisation.  Experimental results on three public datasets show that TAM  not
  only significantly improves supervised learning tasks, including classification
  and regression, but also achieves lower perplexity for the document modeling.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: wang20c
month: 0
tex_title: Neural Topic Model with Attention for Supervised Learning
firstpage: 1147
lastpage: 1156
page: 1147-1156
order: 1147
cycles: false
bibtex_author: Wang, Xinyi and YANG, YI
author:
- given: Xinyi
  family: Wang
- given: YI
  family: YANG
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/wang20c/wang20c.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/wang20c/wang20c-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
