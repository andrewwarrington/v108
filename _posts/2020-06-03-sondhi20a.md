---
title: Balanced Off-Policy Evaluation in General Action Spaces
abstract: Estimation of importance sampling weights for off-policy evaluation of contextual
  bandits often results in imbalanceâ€”a mismatch between the desired and the actual
  distribution of state-action pairs after weighting. In this work we present balanced
  off-policy evaluation (B-OPE), a generic method for estimating weights which minimize
  this imbalance. Estimation of these weights reduces to a binary classification problem
  regardless of action type. We show that minimizing the risk of the classifier implies
  minimization of imbalance to the desired counterfactual distribution. In turn, this
  is tied to the error of the off-policy estimate, allowing for easy tuning of hyperparameters.
  We provide experimental evidence that B-OPE improves weighting-based approaches
  for offline policy evaluation in both discrete and continuous action spaces.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: sondhi20a
month: 0
tex_title: Balanced Off-Policy Evaluation in General Action Spaces
firstpage: 2413
lastpage: 2423
page: 2413-2423
order: 2413
cycles: false
bibtex_author: Sondhi, Arjun and Arbour, David and Dimmery, Drew
author:
- given: Arjun
  family: Sondhi
- given: David
  family: Arbour
- given: Drew
  family: Dimmery
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/sondhi20a/sondhi20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/sondhi20a/sondhi20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
