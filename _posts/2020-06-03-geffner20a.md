---
title: A Rule for Gradient Estimator Selection, with an Application to Variational
  Inference
abstract: Stochastic gradient descent (SGD) is the workhorse of modern machine learning.
  Sometimes, there are many different potential gradient estimators that can be used.
  When so, choosing the one with the best tradeoff between cost and variance is important.
  This paper analyzes the convergence rates of SGD as a function of time, rather than
  iterations. This results in a simple rule to select the estimator that leads to
  the best optimization convergence guarantee. This choice is the same for different
  variants of SGD, and with different assumptions about the objective (e.g. convexity
  or smoothness). Inspired by this principle, we propose a technique to automatically
  select an estimator when a finite pool of estimators is given. Then, we extend to
  infinite pools of estimators, where each one is indexed by control variate weights.
  Empirically, automatically choosing an estimator performs comparably to the best
  estimator chosen with hindsight.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: geffner20a
month: 0
tex_title: A Rule for Gradient Estimator Selection, with an Application to Variational
  Inference
firstpage: 1803
lastpage: 1812
page: 1803-1812
order: 1803
cycles: false
bibtex_author: Geffner, Tomas and Domke, Justin
author:
- given: Tomas
  family: Geffner
- given: Justin
  family: Domke
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/geffner20a/geffner20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/geffner20a/geffner20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
