---
title: Risk Bounds for Learning Multiple Components with Permutation-Invariant Losses
abstract: This paper proposes a simple approach to derive efficient error bounds for
  learning multiple components with sparsity-inducing regularization. We show that
  for such regularization schemes, known decompositions of the Rademacher complexity
  over the components can be used in a more efficient manner to result in tighter
  bounds without too much effort. We give examples of application to switching regression
  and center-based clustering/vector quantization. Then, the complete workflow is
  illustrated on the problem of subspace clustering, for which decomposition results
  were not previously available. For all these problems, the proposed approach yields
  risk bounds with mild dependencies on the number of components and completely removes
  this dependence for nonconvex regularization schemes that could not be handled by
  previous methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: lauer20a
month: 0
tex_title: Risk Bounds for Learning Multiple Components with Permutation-Invariant
  Losses
firstpage: 1178
lastpage: 1187
page: 1178-1187
order: 1178
cycles: false
bibtex_author: Lauer, Fabien
author:
- given: Fabien
  family: Lauer
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/lauer20a/lauer20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
