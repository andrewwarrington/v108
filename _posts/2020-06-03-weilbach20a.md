---
title: Structured Conditional Continuous Normalizing Flows for Efficient Amortized
  Inference in Graphical Models
abstract: We exploit minimally faithful inversion of graphical model structures to
  specify sparse continuous normalizing flows (CNFs) for amortized inference. We find
  that the sparsity of this factorization can be exploited to reduce the numbers of
  parameters in the neural network, adaptive integration steps of the flow, and consequently
  FLOPs at both training and inference time without decreasing performance in comparison
  to unconstrained flows. By expressing the structure inversion as a compilation pass
  in a probabilistic programming language, we are able to apply it in a novel way
  to models as complex as convolutional neural networks. Furthermore, we extend the
  training objective for CNFs in the context of inference amortization to the symmetric
  Kullback-Leibler divergence, and demonstrate its theoretical and practical advantages.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: weilbach20a
month: 0
tex_title: Structured Conditional Continuous Normalizing Flows for Efficient Amortized
  Inference in Graphical Models
firstpage: 4441
lastpage: 4451
page: 4441-4451
order: 4441
cycles: false
bibtex_author: Weilbach, Christian and Beronov, Boyan and Wood, Frank and Harvey,
  William
author:
- given: Christian
  family: Weilbach
- given: Boyan
  family: Beronov
- given: Frank
  family: Wood
- given: William
  family: Harvey
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/weilbach20a/weilbach20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/weilbach20a/weilbach20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
