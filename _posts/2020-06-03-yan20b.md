---
title: Amortized Inference of Variational Bounds for Learning Noisy-OR
abstract: Classical approaches for approximate inference depend on cleverly designed
  variational distributions and bounds. Modern approaches employ amortized variational
  inference, which uses a neural network to approximate any posterior without leveraging  the
  structures of the generative models. In this paper, we propose Amortized Conjugate
  Posterior (ACP), a hybrid approach taking advantages of both types of approaches.
  Specifically, we use the classical methods to derive specific forms of posterior
  distributions and then learn the variational parameters using amortized inference.
  We study the effectiveness of the proposed approach on  the Noisy-OR model  and
  compare to both the classical and the modern approaches for approximate inference
  and parameter learning. Our results show that the proposed method outperforms or
  are at par with other approaches.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: yan20b
month: 0
tex_title: Amortized Inference of Variational Bounds for Learning Noisy-OR
firstpage: 3632
lastpage: 3641
page: 3632-3641
order: 3632
cycles: false
bibtex_author: Yan, Yiming and Ailem, Melissa and Sha, Fei
author:
- given: Yiming
  family: Yan
- given: Melissa
  family: Ailem
- given: Fei
  family: Sha
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/yan20b/yan20b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/yan20b/yan20b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
