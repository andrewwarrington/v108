---
title: A Unified Stochastic Gradient Approach to Designing Bayesian-Optimal Experiments
abstract: We introduce a fully stochastic gradient based approach to Bayesian optimal
  experimental design (BOED). Our approach utilizes variational lower bounds on the
  expected information gain (EIG) of an experiment that can be simultaneously optimized
  with respect to both the variational and design parameters. This allows the design
  process to be carried out through a single unified stochastic gradient ascent procedure,
  in contrast to existing approaches that typically construct a pointwise EIG estimator,
  before passing this estimator to a separate optimizer. We provide a number of different
  variational objectives including the novel adaptive contrastive estimation (ACE)
  bound. Finally, we show that our gradient-based approaches are able to provide effective
  design optimization in substantially higher dimensional settings than existing approaches.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: foster20a
month: 0
tex_title: A Unified Stochastic Gradient Approach to Designing Bayesian-Optimal Experiments
firstpage: 2959
lastpage: 2969
page: 2959-2969
order: 2959
cycles: false
bibtex_author: Foster, Adam and Jankowiak, Martin and O'Meara, Matthew and Teh, Yee
  Whye and Rainforth, Tom
author:
- given: Adam
  family: Foster
- given: Martin
  family: Jankowiak
- given: Matthew
  family: Oâ€™Meara
- given: Yee Whye
  family: Teh
- given: Tom
  family: Rainforth
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/foster20a/foster20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/foster20a/foster20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
