---
title: A Double Residual Compression Algorithm for Efficient Distributed Learning
abstract: Large-scale machine learning models are often trained by parallel stochastic
  gradient descent algorithms. However, the communication cost of gradient aggregation
  and model synchronization between the master and worker nodes becomes the major
  obstacle for efficient learning as the number of workers and the dimension of the
  model increase. In this paper, we propose DORE, a DOuble REsidual compression stochastic
  gradient descent algorithm, to reduce over $95%$ of the overall communication such
  that the obstacle can be immensely mitigated. Our theoretical analyses demonstrate
  that the proposed strategy has superior convergence properties for both strongly
  convex and nonconvex objective functions. The experimental results validate that
  DORE achieves the best communication efficiency while maintaining similar model
  accuracy and convergence speed in comparison with start-of-the-art baselines.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: liu20a
month: 0
tex_title: A Double Residual Compression Algorithm for Efficient Distributed Learning
firstpage: 133
lastpage: 143
page: 133-143
order: 133
cycles: false
bibtex_author: Liu, Xiaorui and Li, Yao and Tang, Jiliang and Yan, Ming
author:
- given: Xiaorui
  family: Liu
- given: Yao
  family: Li
- given: Jiliang
  family: Tang
- given: Ming
  family: Yan
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/liu20a/liu20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/liu20a/liu20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
