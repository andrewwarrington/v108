---
title: Understanding the Intrinsic Robustness of Image Distributions using Conditional
  Generative Models
abstract: Starting with Gilmer et al. (2018), several works have demonstrated the
  inevitability of adversarial examples based on different assumptions about the underlying
  input probability space.  It remains unclear, however, whether these results apply
  to natural image distributions. In this work, we assume the underlying data distribution
  is captured by some conditional generative model, and prove intrinsic robustness
  bounds for a general class of classifiers, which solves an open problem in Fawzi
  et al. (2018). Building upon the state-of-the-art conditional generative models,
  we study the intrinsic robustness of two common image benchmarks under L2 perturbations,
  and show the existence of a large gap between the robustness limits implied by our
  theory and the adversarial robustness achieved by current state-of-the-art robust
  models.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zhang20h
month: 0
tex_title: Understanding the Intrinsic Robustness of Image Distributions using Conditional
  Generative Models
firstpage: 3883
lastpage: 3893
page: 3883-3893
order: 3883
cycles: false
bibtex_author: Zhang, Xiao and Chen, Jinghui and Gu, Quanquan and Evans, David
author:
- given: Xiao
  family: Zhang
- given: Jinghui
  family: Chen
- given: Quanquan
  family: Gu
- given: David
  family: Evans
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/zhang20h/zhang20h.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/zhang20h/zhang20h-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
