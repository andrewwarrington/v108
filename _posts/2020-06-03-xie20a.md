---
title: Linear Convergence of Adaptive Stochastic Gradient Descent
abstract: We prove that the norm version of the adaptive stochastic gradient method
  (AdaGrad-Norm) achieves a linear convergence rate for a subset of either strongly
  convex functions or non-convex functions that satisfy the Polyak Lojasiewicz (PL)
  inequality. The paper introduces the notion of Restricted Uniform Inequality of
  Gradients (RUIG)—which is a measure of the balanced-ness of the stochastic gradient
  norms—to depict the landscape of a function. RUIG plays a key role in proving the
  robustness of AdaGrad-Norm to its hyper-parameter tuning in the stochastic setting.
  On top of RUIG, we develop a two-stage framework to prove the linear convergence
  of AdaGrad-Norm without knowing the parameters of the objective functions. This
  framework can likely be extended to other adaptive stepsize algorithms. The numerical
  experiments validate the theory and suggest future directions for improvement.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: xie20a
month: 0
tex_title: Linear Convergence of Adaptive Stochastic Gradient Descent
firstpage: 1475
lastpage: 1485
page: 1475-1485
order: 1475
cycles: false
bibtex_author: Xie, Yuege and Wu, Xiaoxia and Ward, Rachel
author:
- given: Yuege
  family: Xie
- given: Xiaoxia
  family: Wu
- given: Rachel
  family: Ward
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/xie20a/xie20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/xie20a/xie20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
