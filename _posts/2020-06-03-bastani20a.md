---
title: Sample Complexity of Estimating the Policy Gradient for Nearly Deterministic
  Dynamical Systems
abstract: Reinforcement learning is a promising approach to learning robotics controllers.
  It has recently been shown that algorithms based on finite-difference estimates
  of the policy gradient are competitive with algorithms based on the policy gradient
  theorem. We propose a theoretical framework for understanding this phenomenon. Our
  key insight is that many dynamical systems (especially those of interest in robotics
  control tasks) are nearly deterministicâ€”i.e., they can be modeled as a deterministic
  system with a small stochastic perturbation. We show that for such systems, finite-difference
  estimates of the policy gradient can have substantially lower variance than estimates
  based on the policy gradient theorem. Finally, we empirically evaluate our insights
  in an experiment on the inverted pendulum.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: bastani20a
month: 0
tex_title: Sample Complexity of Estimating the Policy Gradient for Nearly Deterministic
  Dynamical Systems
firstpage: 3858
lastpage: 3869
page: 3858-3869
order: 3858
cycles: false
bibtex_author: Bastani, Osbert
author:
- given: Osbert
  family: Bastani
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/bastani20a/bastani20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/bastani20a/bastani20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
