---
title: Doubly Sparse Variational Gaussian Processes
abstract: 'The use of Gaussian process models is typically limited to datasets with
  a few tens of thousands of observations due to their complexity and memory footprint.The
  two most commonly used methods to overcome this limitation are 1) the variational
  sparse approximation which relies on inducing points and 2) the state-space equivalent
  formulation of Gaussian processes which can be seen as exploiting some sparsity
  in the precision matrix.In this work, we propose to take the best of both worlds:
  we show that the inducing point framework is still valid for state space models
  and that it can bring further computational and memory savings. Furthermore, we
  provide the natural gradient formulation for the proposed variational parameterisation.Finally,
  this work makes it possible to use the state-space formulation inside deep Gaussian
  process models as illustrated in one of the experiments. '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: adam20a
month: 0
tex_title: Doubly Sparse Variational Gaussian Processes
firstpage: 2874
lastpage: 2884
page: 2874-2884
order: 2874
cycles: false
bibtex_author: Adam, Vincent and Eleftheriadis, Stefanos and Artemev, Artem and Durrande,
  Nicolas and Hensman, James
author:
- given: Vincent
  family: Adam
- given: Stefanos
  family: Eleftheriadis
- given: Artem
  family: Artemev
- given: Nicolas
  family: Durrande
- given: James
  family: Hensman
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/adam20a/adam20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/adam20a/adam20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
