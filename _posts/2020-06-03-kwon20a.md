---
title: EM Converges for a Mixture of Many Linear Regressions
abstract: We study the convergence of the Expectation-Maximization (EM) algorithm
  for mixtures of linear regressions with an arbitrary number $k$ of components. We
  show that as long as signal-to-noise ratio (SNR) is $\tilde{\Omega}(k)$, well-initialized
  EM converges to the true regression parameters. Previous results for $k \geq 3$
  have only established local convergence for the noiseless setting, i.e., where SNR
  is infinitely large. Our results enlarge the scope to the environment with noises,
  and notably, we establish a statistical error rate that is independent of the norm
  (or pairwise distance) of the regression parameters. In particular, our results
  imply exact recovery as $\sigma \rightarrow 0$, in contrast to most previous local
  convergence results for EM, where the statistical error scaled with the norm of
  parameters. Standard moment-method approaches may be applied to guarantee we are
  in the region where our local convergence guarantees apply.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: kwon20a
month: 0
tex_title: EM Converges for a Mixture of Many Linear Regressions
firstpage: 1727
lastpage: 1736
page: 1727-1736
order: 1727
cycles: false
bibtex_author: Kwon, Jeongyeol and Caramanis, Constantine
author:
- given: Jeongyeol
  family: Kwon
- given: Constantine
  family: Caramanis
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/kwon20a/kwon20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/kwon20a/kwon20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
