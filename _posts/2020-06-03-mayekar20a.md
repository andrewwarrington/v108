---
title: 'RATQ: A Universal Fixed-Length Quantizer for Stochastic Optimization'
abstract: We present Rotated Adaptive Tetra-iterated Quantizer (RATQ), afixed-length
  quantizer for gradients in first order stochasticoptimization.  RATQ is easy to
  implement and involves only a Hadamard transform computation and adaptive uniform
  quantization with appropriately chosen dynamic ranges. For noisy gradients with
  almost surely bounded Euclidean norms, we establish an informationtheoretic lower
  bound for optimization accuracy using finite precisiongradients and show that RATQ
  almost attains this lower bound.  For mean square bounded noisy gradients, we use
  a                                                             gain-shape quantizer
  which separately quantizes the Euclidean norm and uses RATQ to quantize the normalized
  unit norm vector. We establish lower bounds for performance of any optimization
  procedure and shape quantizer, when used with a uniform gain quantizer. Finally,
  we propose an adaptive quantizer for gain which when used with RATQ for shape quantizer
  outperforms uniform gain quantization and is, in fact, close to optimal. Â 
layout: inproceedings
series: Proceedings of Machine Learning Research
id: mayekar20a
month: 0
tex_title: 'RATQ: A Universal Fixed-Length Quantizer for Stochastic Optimization'
firstpage: 1399
lastpage: 1409
page: 1399-1409
order: 1399
cycles: false
bibtex_author: Mayekar, Prathamesh and Tyagi, Himanshu
author:
- given: Prathamesh
  family: Mayekar
- given: Himanshu
  family: Tyagi
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/mayekar20a/mayekar20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
