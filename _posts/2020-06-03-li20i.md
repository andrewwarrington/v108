---
title: Scalable Gradients for Stochastic Differential Equations
abstract: The adjoint sensitivity method scalably computes gradients of solutions
  to ordinary differential equations. We generalize this method to stochastic differential
  equations, allowing time-efficient and constant-memory computation of gradients
  with high-order adaptive solvers. Specifically, we derive a stochastic differentialequation
  whose solution is the gradient, a memory-efficient algorithm for cachingnoise, and
  conditions under which numerical solutions converge. In addition, we combine our
  method with gradient-based stochastic variational inference for latent stochastic
  differential equations. We use our method to fit stochastic dynamics defined by
  neural networks, achieving competitive performance ona 50-dimensional motion capture
  dataset.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: li20i
month: 0
tex_title: Scalable Gradients for Stochastic Differential Equations
firstpage: 3870
lastpage: 3882
page: 3870-3882
order: 3870
cycles: false
bibtex_author: Li, Xuechen and Wong, Ting-Kam Leonard and Chen, Ricky T. Q. and Duvenaud,
  David
author:
- given: Xuechen
  family: Li
- given: Ting-Kam Leonard
  family: Wong
- given: Ricky T. Q.
  family: Chen
- given: David
  family: Duvenaud
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/li20i/li20i.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/li20i/li20i-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
