---
title: Automatic Differentiation of Some First-Order Methods in Parametric Optimization
abstract: We aim at computing the derivative of the solution to a parametric optimization
  problem with respect to the involved parameters. For a class broader than that of
  strongly convex functions, this can be achieved by automatic differentiation of
  iterative minimization algorithms. If the iterative algorithm converges pointwise,
  then we prove that the derivative sequence also converges pointwise to the derivative
  of the minimizer with respect to the parameters. Moreover, we provide convergence
  rates for both sequences. In particular, we prove that the accelerated convergence
  rate of the Heavy-ball method compared to Gradient Descent also accelerates the
  derivative computation. An experiment with L2-Regularized Logistic Regression validates
  the theoretical results.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: mehmood20a
month: 0
tex_title: Automatic Differentiation of Some First-Order Methods in Parametric Optimization
firstpage: 1584
lastpage: 1594
page: 1584-1594
order: 1584
cycles: false
bibtex_author: Mehmood, Sheheryar and Ochs, Peter
author:
- given: Sheheryar
  family: Mehmood
- given: Peter
  family: Ochs
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/mehmood20a/mehmood20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/mehmood20a/mehmood20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
