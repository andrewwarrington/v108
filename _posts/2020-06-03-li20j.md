---
title: Gradient Descent with Early Stopping is Provably Robust to Label Noise for
  Overparameterized Neural Networks
abstract: 'Modern neural networks are typically trained in an over-parameterized regime
  where the parameters of the model far exceed the size of the training data. Such
  neural networks in principle have the capacity to (over)fit any set of labels including
  significantly corrupted ones. Despite this (over)fitting capacity in this paper
  we demonstrate that such overparameterized networks have an intriguing robustness
  capability: they are surprisingly robust to label noise when first order methods
  with early stopping is used to train them. This paper also takes a step towards
  demystifying this phenomena. Under a rich dataset model, we show that gradient descent
  is provably robust to noise/corruption on a constant fraction of the labels. In
  particular, we prove that: (i) In the first few iterations where the updates are
  still in the vicinity of the initialization gradient descent only fits to the correct
  labels essentially ignoring the noisy labels. (ii) To start to overfit to the noisy
  labels network must stray rather far from the initialization which can only occur
  after many more iterations. Together, these results show that gradient descent with
  early stopping is provably robust to label noise and shed light on the empirical
  robustness of deep networks as well as commonly adopted heuristics to prevent overfitting.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: li20j
month: 0
tex_title: Gradient Descent with Early Stopping is Provably Robust to Label Noise
  for Overparameterized Neural Networks
firstpage: 4313
lastpage: 4324
page: 4313-4324
order: 4313
cycles: false
bibtex_author: Li, Mingchen and Soltanolkotabi, Mahdi and Oymak, Samet
author:
- given: Mingchen
  family: Li
- given: Mahdi
  family: Soltanolkotabi
- given: Samet
  family: Oymak
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/li20j/li20j.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/li20j/li20j-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
