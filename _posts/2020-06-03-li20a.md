---
title: On the Convergence of SARAH and Beyond
abstract: 'The main theme of this work is a unifying algorithm, \textbf{L}oop\textbf{L}ess
  \textbf{S}ARAH (L2S) for problems formulated as summation of $n$ individual loss
  functions. L2S broadens a recently developed variance reduction method known as
  SARAH. To find an $\epsilon$-accurate solution, L2S enjoys a complexity of ${\cal
  O}\big( (n+\kappa) \ln (1/\epsilon)\big)$ for strongly convex problems. For convex
  problems, when adopting an $n$-dependent step size, the complexity of L2S is ${\cal
  O}(n+ \sqrt{n}/\epsilon)$; while for more frequently adopted $n$-independent step
  size, the complexity is ${\cal O}(n+ n/\epsilon)$. Distinct from SARAH, our theoretical
  findings support an $n$-independent step size in convex problems without extra assumptions.
  For nonconvex problems, the complexity of L2S is ${\cal O}(n+ \sqrt{n}/\epsilon)$.
  Our numerical tests on neural networks suggest that L2S can have better generalization
  properties than SARAH. Along with L2S, our side results include the linear convergence
  of the last iteration for SARAH in strongly convex problems. '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: li20a
month: 0
tex_title: On the Convergence of SARAH and Beyond
firstpage: 223
lastpage: 233
page: 223-233
order: 223
cycles: false
bibtex_author: Li, Bingcong and Ma, Meng and Giannakis, Georgios B.
author:
- given: Bingcong
  family: Li
- given: Meng
  family: Ma
- given: Georgios B.
  family: Giannakis
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/li20a/li20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/li20a/li20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
