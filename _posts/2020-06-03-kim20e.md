---
title: Stochastic Variance-Reduced Algorithms for PCA with Arbitrary Mini-Batch Sizes
abstract: We present two stochastic variance-reduced PCA algorithms and their convergence
  analyses. By deriving explicit forms of step size, epoch length and batch size to
  ensure the optimal runtime, we show that the proposed algorithms can attain the
  optimal runtime with any batch sizes. Also, we establish global convergence of the
  algorithms based on a novel approach, which studies the optimality gap as a ratio
  of two expectation terms. The framework in our analysis is general and can be used
  to analyze other stochastic variance-reduced PCA algorithms and improve their analyses.
  Moreover, we introduce practical implementations of the algorithms which do not
  require hyper-parameters. The experimental results show that the proposed methodsd
  outperform other stochastic variance-reduced PCA algorithms regardless of the batch
  size.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: kim20e
month: 0
tex_title: Stochastic Variance-Reduced Algorithms for PCA with Arbitrary Mini-Batch
  Sizes
firstpage: 4302
lastpage: 4312
page: 4302-4312
order: 4302
cycles: false
bibtex_author: Kim, Cheolmin and Klabjan, Diego
author:
- given: Cheolmin
  family: Kim
- given: Diego
  family: Klabjan
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/kim20e/kim20e.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/kim20e/kim20e-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
