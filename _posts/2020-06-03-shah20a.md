---
title: Choosing the Sample with Lowest Loss makes SGD Robust
abstract: 'The presence of outliers can potentially significantly skew the parameters
  of machine learning models trained via stochastic gradient descent (SGD). In this
  paper we propose a simple variant of the simple SGD method: in each step, first
  choose a set of k samples, then from these choose the one with the smallest current
  loss, and do an SGD-like update with this chosen sample. Vanilla SGD corresponds
  to $k=1$, i.e. no choice; $k>=2$ represents a new algorithm that is however effectively
  minimizing a non-convex surrogate loss. Our main contribution is a theoretical analysis
  of the robustness properties of this idea for ML problems which are sums of convex
  losses; these are backed up with synthetic and small-scale neural network experiments.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: shah20a
month: 0
tex_title: Choosing the Sample with Lowest Loss makes SGD Robust
firstpage: 2120
lastpage: 2130
page: 2120-2130
order: 2120
cycles: false
bibtex_author: Shah, Vatsal and Wu, Xiaoxia and Sanghavi, Sujay
author:
- given: Vatsal
  family: Shah
- given: Xiaoxia
  family: Wu
- given: Sujay
  family: Sanghavi
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/shah20a/shah20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/shah20a/shah20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
