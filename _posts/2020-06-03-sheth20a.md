---
title: Differentiable Feature Selection by Discrete Relaxation
abstract: In this paper, we introduce Differentiable Feature Selection, a gradient-based
  search algorithm for feature selection. Our approach extends a recent result on
  the estimation of learnability in the sublinear data regime by showing that the
  calculation can be performed iteratively (i.e. in mini-batches) and in linear time
  and space with respect to both the number of features D and the sample size N. This,
  along with a discrete-to-continuous relaxation of the search domain, allows for
  an efficient, gradient-based search algorithm among feature subsets for very large
  datasets. Our algorithm utilizes higher-order correlations between features and
  targets for both the N>D and N<D regimes, as opposed to approaches that do not consider
  such interactions and/or only consider one regime. We provide experimental demonstration
  of the algorithm in small and large sample- and feature-size settings.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: sheth20a
month: 0
tex_title: Differentiable Feature Selection by Discrete Relaxation
firstpage: 1564
lastpage: 1572
page: 1564-1572
order: 1564
cycles: false
bibtex_author: Sheth, Rishit and Fusi, Nicol\'o
author:
- given: Rishit
  family: Sheth
- given: NicolÃ³
  family: Fusi
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/sheth20a/sheth20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
