---
title: 'Beyond exploding and vanishing gradients: analysing RNN training using attractors
  and smoothness'
abstract: The exploding and vanishing gradient problem has been the major conceptual
  principle behind most architecture and training improvements in recurrent neural
  networks (RNNs) during the last decade.  In this paper, we argue that this principle,
  while powerful, might need some refinement to explain recent developments. We refine
  the concept of exploding gradients by reformulating the problem in terms of the
  cost function smoothness, which gives insight into higher-order derivatives and
  the existence of regions with many close local minima. We also clarify the distinction
  between vanishing gradients and the need for the RNN to learn attractors to fully
  use its expressive power. Through the lens of these refinements, we shed new light
  on recent developments in the RNN field, namely stable RNN and unitary (or orthogonal)
  RNNs.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: ribeiro20a
month: 0
tex_title: 'Beyond exploding and vanishing gradients: analysing RNN training using
  attractors and smoothness'
firstpage: 2370
lastpage: 2380
page: 2370-2380
order: 2370
cycles: false
bibtex_author: Ribeiro, Ant\'onio H. and Tiels, Koen and Aguirre, Luis A. and Sch\"on,
  Thomas
author:
- given: António H.
  family: Ribeiro
- given: Koen
  family: Tiels
- given: Luis A.
  family: Aguirre
- given: Thomas
  family: Schön
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/ribeiro20a/ribeiro20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/ribeiro20a/ribeiro20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
