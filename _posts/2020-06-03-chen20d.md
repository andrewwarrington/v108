---
title: On Generalization Bounds of a Family of Recurrent Neural Networks
abstract: 'Recurrent Neural Networks (RNNs) have been widely applied to sequential
  data analysis. Due to their complicated modeling structures, however, the theory
  behind is still largely missing. To connect theory and practice, we study the generalization
  properties of vanilla RNNs as well as their variants, including Minimal Gated Unit
  (MGU), Long Short Term Memory (LSTM), and Convolutional (Conv) RNNs. Specifically,
  our theory is established under the PAC-Learning framework. The generalization bound
  is presented in terms of the spectral norms of the weight matrices and the total
  number of parameters. We also establish refined generalization bounds with additional
  norm assumptions, and draw a comparison among these bounds. We remark: (1) Our generalization
  bound for vanilla RNNs is significantly tighter than the best of existing results;
  (2) We are not aware of any other generalization bounds for MGU and LSTM RNNs in
  the exiting literature; (3) We demonstrate the advantages of these variants in generalization.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: chen20d
month: 0
tex_title: On Generalization Bounds of a Family of Recurrent Neural Networks
firstpage: 1233
lastpage: 1243
page: 1233-1243
order: 1233
cycles: false
bibtex_author: Chen, Minshuo and Li, Xingguo and Zhao, Tuo
author:
- given: Minshuo
  family: Chen
- given: Xingguo
  family: Li
- given: Tuo
  family: Zhao
date: 2020-06-03
address: 
publisher: PMLR
container-title: Proceedings of the Twenty Third International Conference on Artificial
  Intelligence and Statistics
volume: '108'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 6
  - 3
pdf: http://proceedings.mlr.press/v108/chen20d/chen20d.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v108/chen20d/chen20d-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
